{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23590014-5638-40f8-a4e4-6f8141c09052",
   "metadata": {},
   "source": [
    "# Training a neural network on each example from the DeepCME paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbda44-b8a5-4892-bbb1-dfd85e5256f8",
   "metadata": {},
   "source": [
    "For each CRN, the following configuration holds:\n",
    "- End time: 1 second\n",
    "- Time steps: 50\n",
    "- Initial concentration for all species: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f477f-f71b-418a-b2c4-ce6e4049642d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b2627-5398-4c25-938d-1cc37e1dd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def generate_colours(num_colours):\n",
    "    colours = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                 for i in range(num_colours)]\n",
    "    return colours\n",
    "\n",
    "COLOURS = generate_colours(50)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7dac5-e5a8-4052-bb4e-c8f29f3a28c4",
   "metadata": {},
   "source": [
    "## Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0c538-46e7-44ed-b7ca-33c0d847edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic to keep track of file changes in real-time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Core imports\n",
    "import os, sys, json, h5py, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from importlib import import_module\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# StochNetV2 library imports (unchanged files from the framework)\n",
    "from stochnet_v2.dataset.dataset import DataTransformer, HDF5Dataset\n",
    "\n",
    "from stochnet_v2.static_classes.model import StochNet\n",
    "from stochnet_v2.static_classes.trainer import ToleranceDropLearningStrategy\n",
    "\n",
    "from stochnet_v2.dynamic_classes.model import NASStochNet\n",
    "from stochnet_v2.dynamic_classes.trainer import Trainer\n",
    "\n",
    "from stochnet_v2.utils.file_organisation import ProjectFileExplorer\n",
    "from stochnet_v2.utils.util import merge_species_and_param_settings, plot_random_traces, visualize_genotypes\n",
    "\n",
    "# StochNetV2 local imports (files that have been modified)\n",
    "from simulation_gillespy import build_simulation_dataset\n",
    "from util import generate_gillespy_traces\n",
    "from evaluation import evaluate\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"GPU is available: {tf.test.is_built_with_cuda(), tf.test.is_gpu_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51bd86-39bd-4673-98bf-2e13b9fac47b",
   "metadata": {},
   "source": [
    "## Define CRNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7bed5b-e578-4347-9d96-78eeeb4bdd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (network name, number of species, [params to randomise])\n",
    "networks = [\n",
    "    ('BD5', 5, ['k', 'gamma']),\n",
    "    ('BD10', 10, ['k', 'gamma']),\n",
    "    ('BD20', 20, ['k', 'gamma']),\n",
    "    ('LSC2', 2, ['beta_0', 'k', 'gamma']),\n",
    "    ('LSC5', 5, ['beta_0', 'k', 'gamma']),\n",
    "    ('LSC10', 10, ['beta_0', 'k', 'gamma']),\n",
    "    ('NSC2', 2, ['b', 'k_m', 'k_0', 'H', 'beta_0', 'gamma']),\n",
    "    ('NSC5', 5, ['b', 'k_m', 'k_0', 'H', 'beta_0', 'gamma']),\n",
    "    ('NSC10', 10, ['b', 'k_m', 'k_0', 'H', 'beta_0', 'gamma']),\n",
    "    ('LSCF2', 2, ['b', 'k_m', 'k_0', 'H', 'k', 'gamma']),\n",
    "    ('LSCF5', 5, ['b', 'k_m', 'k_0', 'H', 'k', 'gamma']),\n",
    "    ('LSCF10', 10, ['b', 'k_m', 'k_0', 'H', 'k', 'gamma'])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd3730-2e64-4a77-a729-4d70eda1821b",
   "metadata": {},
   "source": [
    "## Train NNs on 1000-trace datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f84c1-fa65-40ac-ba74-4e45b9431ed3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (name, n_species, params) in enumerate(networks):\n",
    "    print(f\">>> Working with model {name}.\")\n",
    "    # Configure the model parameters\n",
    "    model_name = name\n",
    "    timestep = 0.02\n",
    "    endtime = 1.0 \n",
    "    dataset_id = name\n",
    "    model_id = name\n",
    "    nb_features = n_species\n",
    "    params_to_randomize = params\n",
    "\n",
    "    # Configure the simulation parameters\n",
    "    nb_settings = 500\n",
    "    nb_trajectories = 10\n",
    "\n",
    "    nb_histogram_settings = 500\n",
    "    nb_histogram_trajectories = 10\n",
    "\n",
    "    # File-handling and housekeeping\n",
    "    project_folder = Path('').parent.resolve()/model_name\n",
    "    project_explorer = ProjectFileExplorer(project_folder)\n",
    "    dataset_explorer = project_explorer.get_dataset_file_explorer(timestep, dataset_id)\n",
    "    model_explorer = project_explorer.get_model_file_explorer(timestep, model_id)\n",
    "\n",
    "    body_config_path = model_explorer.body_config_fp\n",
    "    mixture_config_path = model_explorer.mixture_config_fp\n",
    "\n",
    "    CRN_module = import_module(model_name)\n",
    "    CRN_class = getattr(CRN_module, model_name)\n",
    "\n",
    "    # Generate and save the initial species concentrations\n",
    "    settings = CRN_class.get_initial_settings(nb_settings)\n",
    "    print(f\"Settings shape: {settings.shape}\")\n",
    "    print(f\"Saving settings to {dataset_explorer.settings_fp}\\n\")\n",
    "    np.save(dataset_explorer.settings_fp, settings)\n",
    "\n",
    "    histogram_settings = CRN_class.get_initial_settings(nb_histogram_settings)\n",
    "    print(f\"Histogram settings shape: {histogram_settings.shape}\")\n",
    "    print(f\"Saving histogram_settings to {dataset_explorer.histogram_settings_fp}\")\n",
    "    np.save(dataset_explorer.histogram_settings_fp, histogram_settings)\n",
    "\n",
    "    # Generate the dataset of trajectories of shape (n_settings * n_trajectories, n_steps, n_species)\n",
    "    dataset = build_simulation_dataset(\n",
    "        model_name,                              \n",
    "        nb_settings,                            \n",
    "        nb_trajectories,                         \n",
    "        timestep,                                \n",
    "        endtime,                                 \n",
    "        dataset_explorer.dataset_folder,         \n",
    "        params_to_randomize=params_to_randomize,\n",
    "        how='concat'\n",
    "    )\n",
    "\n",
    "    np.save(dataset_explorer.dataset_fp, dataset)\n",
    "\n",
    "    # Generate the histogram dataset of shape (n_settings, n_trajectories, n_steps, n_species)\n",
    "    histogram_dataset = build_simulation_dataset(\n",
    "        model_name,\n",
    "        nb_histogram_settings,\n",
    "        nb_histogram_trajectories,\n",
    "        timestep,\n",
    "        endtime,\n",
    "        dataset_explorer.dataset_folder,\n",
    "        params_to_randomize=params_to_randomize,\n",
    "        prefix='histogram_partial_',\n",
    "        how='stack',\n",
    "        settings_filename=os.path.basename(dataset_explorer.histogram_settings_fp),\n",
    "    )\n",
    "\n",
    "    np.save(dataset_explorer.histogram_dataset_fp, histogram_dataset)\n",
    "\n",
    "    # Transform the dataset\n",
    "    dt = DataTransformer(\n",
    "        dataset_explorer.dataset_fp,\n",
    "        with_timestamps=True,\n",
    "        nb_randomized_params=len(params_to_randomize)\n",
    "    )\n",
    "\n",
    "    # Convert and save as HDF5\n",
    "    dt.save_data_for_ml_hdf5(\n",
    "        dataset_folder=dataset_explorer.dataset_folder,\n",
    "        nb_past_timesteps=1,\n",
    "        test_fraction=0.2,\n",
    "        keep_timestamps=False,\n",
    "        rescale=True,\n",
    "        positivity=False,\n",
    "        shuffle=True,\n",
    "        slice_size=100,\n",
    "        force_rewrite=True\n",
    "    )\n",
    "    \n",
    "    # Architecture parameters\n",
    "    body_n_cells = 2\n",
    "    body_cell_size = 2\n",
    "    body_expansion_multiplier = 20\n",
    "    body_n_states_reduce = 2\n",
    "    body_kernel_constraint = \"none\"\n",
    "    body_bias_constraint = \"none\"\n",
    "    body_kernel_regularizer = \"l2\"\n",
    "    body_bias_regularizer = \"l2\"\n",
    "    body_regularizer = \"none\"\n",
    "\n",
    "    components_hidden_size = \"none\"\n",
    "    n_normal_diag = 6\n",
    "    n_normal_tril = 0\n",
    "    n_log_normal_tril = 0\n",
    "    components_activation = \"none\"\n",
    "    components_regularizer = \"none\"\n",
    "    components_kernel_constraint = \"none\"\n",
    "    components_bias_constraint = \"none\"\n",
    "    components_kernel_regularizer = \"l2\"\n",
    "    components_bias_regularizer = \"l2\"\n",
    "\n",
    "    # Architecture configurations\n",
    "    body_config = {\n",
    "        \"n_cells\": body_n_cells,\n",
    "        \"cell_size\": body_cell_size,\n",
    "        \"expansion_multiplier\": body_expansion_multiplier,\n",
    "        \"n_states_reduce\": body_n_states_reduce,\n",
    "        \"kernel_constraint\": body_kernel_constraint,\n",
    "        \"kernel_regularizer\": body_kernel_regularizer,\n",
    "        \"bias_constraint\": body_bias_constraint,\n",
    "        \"bias_regularizer\": body_bias_regularizer,\n",
    "        \"activity_regularizer\": body_regularizer,\n",
    "    }\n",
    "\n",
    "    categorical_config = {\n",
    "        \"hidden_size\": components_hidden_size,\n",
    "        \"activation\": components_activation,\n",
    "        \"coeff_regularizer\": \"none\",\n",
    "        \"kernel_constraint\": body_kernel_constraint,  # unitnorm\n",
    "        \"bias_constraint\": body_bias_constraint,  # unitnorm\n",
    "        \"kernel_regularizer\": components_kernel_regularizer,\n",
    "        \"bias_regularizer\": components_bias_regularizer\n",
    "    }\n",
    "\n",
    "    normal_diag_config = {\n",
    "        \"hidden_size\": components_hidden_size,\n",
    "        \"activation\": components_activation,\n",
    "        \"mu_regularizer\": components_regularizer,\n",
    "        \"diag_regularizer\": \"l2\",\n",
    "        \"kernel_constraint\": components_kernel_constraint,\n",
    "        \"bias_constraint\": components_bias_constraint,\n",
    "        \"kernel_regularizer\": components_kernel_regularizer,\n",
    "        \"bias_regularizer\": components_bias_regularizer\n",
    "    }\n",
    "\n",
    "    normal_tril_config = {\n",
    "        \"hidden_size\": components_hidden_size,\n",
    "        \"activation\": components_activation,\n",
    "        \"mu_regularizer\": components_regularizer,\n",
    "        \"diag_regularizer\": components_regularizer,\n",
    "        \"sub_diag_regularizer\": components_regularizer,\n",
    "        \"kernel_constraint\": components_kernel_constraint,\n",
    "        \"bias_constraint\": components_bias_constraint,\n",
    "        \"kernel_regularizer\": components_kernel_regularizer,\n",
    "        \"bias_regularizer\": components_bias_regularizer\n",
    "    }\n",
    "\n",
    "    log_normal_tril_config = {\n",
    "        \"hidden_size\": components_hidden_size,\n",
    "        \"activation\": components_activation,\n",
    "        \"mu_regularizer\": components_regularizer,\n",
    "        \"diag_regularizer\": components_regularizer,\n",
    "        \"sub_diag_regularizer\": components_regularizer,\n",
    "        \"kernel_constraint\": components_kernel_constraint,\n",
    "        \"bias_constraint\": components_bias_constraint,\n",
    "        \"kernel_regularizer\": components_kernel_regularizer,\n",
    "        \"bias_regularizer\": components_bias_regularizer\n",
    "    }\n",
    "\n",
    "    # Write the configurations to disk\n",
    "    mixture_config = \\\n",
    "    [[\"categorical\", categorical_config]] + \\\n",
    "    [[\"normal_diag\", normal_diag_config] for i in range(n_normal_diag)] + \\\n",
    "    [[\"normal_tril\", normal_tril_config] for i in range(n_normal_tril)] + \\\n",
    "    [[\"log_normal_tril\", log_normal_tril_config] for i in range(n_log_normal_tril)]\n",
    "\n",
    "    with open(body_config_path, 'w+') as f:\n",
    "        json.dump(body_config, f, indent='\\t')\n",
    "\n",
    "    with open(mixture_config_path, 'w+') as f:\n",
    "        json.dump(mixture_config, f, indent='\\t')\n",
    "        \n",
    "    # Training parameters\n",
    "    n_epochs_main = 100\n",
    "    n_epochs_heat_up = 20\n",
    "    n_epochs_interval = 5\n",
    "    n_epochs_arch = 5\n",
    "    n_epochs_finetune = 40\n",
    "    \n",
    "    batch_size = 256\n",
    "    dataset_kind = 'hdf5'\n",
    "    add_noise = False\n",
    "    stddev = 0.01\n",
    "\n",
    "    # Trainng strategy\n",
    "    learning_strategy_main = ToleranceDropLearningStrategy(\n",
    "        optimizer_type='adam',\n",
    "        initial_lr=1e-4,\n",
    "        lr_decay=0.3,\n",
    "        epochs_tolerance=7,\n",
    "        minimal_lr=1e-7,\n",
    "    )\n",
    "\n",
    "    learning_strategy_arch = ToleranceDropLearningStrategy(\n",
    "        optimizer_type='adam',\n",
    "        initial_lr=1e-3,\n",
    "        lr_decay=0.5,\n",
    "        epochs_tolerance=20,\n",
    "        minimal_lr=1e-7,\n",
    "    )\n",
    "\n",
    "    learning_strategy_finetune = ToleranceDropLearningStrategy(\n",
    "        optimizer_type='adam',\n",
    "        initial_lr=1e-4,\n",
    "        lr_decay=0.3,\n",
    "        epochs_tolerance=5,\n",
    "        minimal_lr=1e-7,\n",
    "    )\n",
    "    \n",
    "    nn = NASStochNet(\n",
    "        nb_past_timesteps=1,\n",
    "        nb_features=nb_features,\n",
    "        nb_randomized_params=len(params_to_randomize),\n",
    "        project_folder=project_folder,\n",
    "        timestep=timestep,\n",
    "        dataset_id=dataset_id,\n",
    "        model_id=model_id,\n",
    "    )\n",
    "    \n",
    "    start = time()\n",
    "    ckpt_path = None\n",
    "    ckpt_path = Trainer().train(\n",
    "        nn,\n",
    "        n_epochs_main=n_epochs_main,\n",
    "        n_epochs_heat_up=n_epochs_heat_up,\n",
    "        n_epochs_arch=n_epochs_arch,\n",
    "        n_epochs_interval=n_epochs_interval,\n",
    "        n_epochs_finetune=n_epochs_finetune,\n",
    "        batch_size=batch_size,\n",
    "        learning_strategy_main=learning_strategy_main,\n",
    "        learning_strategy_arch=learning_strategy_arch,\n",
    "        learning_strategy_finetune=learning_strategy_finetune,\n",
    "        ckpt_path=ckpt_path,\n",
    "        dataset_kind=dataset_kind,\n",
    "        add_noise=add_noise,\n",
    "        stddev=stddev,\n",
    "        mode=['search', 'finetune']\n",
    "    )\n",
    "    end = time()\n",
    "    time_taken = end - start\n",
    "    \n",
    "    with open(os.path.join(nn.model_explorer.model_folder, 'genotypes.pickle'), 'rb') as f:\n",
    "        genotypes = pickle.load(f)\n",
    "\n",
    "    visualize_genotypes(genotypes, fr'{name}\\genotypes_{name}')\n",
    "    \n",
    "    distance_kind = 'dist'\n",
    "    target_species_names = [ f'S{i+1}' for i in range(n_species)]\n",
    "    # target_species_names = [ f'S{n_species}']\n",
    "    time_lag_range = [1, 3, 5, 10, 15, 20]\n",
    "    settings_idxs_to_save_histograms = [i for i in range(nb_settings)]\n",
    "\n",
    "    histogram_explorer = dataset_explorer.get_histogram_file_explorer(model_id, 0)\n",
    "    nn_histogram_data_fp = os.path.join(histogram_explorer.model_histogram_folder, 'nn_histogram_data.npy')\n",
    "\n",
    "    evaluate(\n",
    "        model_name=model_name,\n",
    "        project_folder=project_folder,\n",
    "        timestep=timestep,\n",
    "        dataset_id=dataset_id,\n",
    "        model_id=model_id,\n",
    "        nb_randomized_params=len(params_to_randomize),\n",
    "        nb_past_timesteps=1,\n",
    "        n_bins=100,\n",
    "        distance_kind=distance_kind,\n",
    "        with_timestamps=True,\n",
    "        save_histograms=True,\n",
    "        time_lag_range=time_lag_range,\n",
    "        target_species_names=target_species_names,\n",
    "        path_to_save_nn_traces=nn_histogram_data_fp,\n",
    "        settings_idxs_to_save_histograms=settings_idxs_to_save_histograms,\n",
    "    )\n",
    "    \n",
    "    # Initialise the network and corresponding parameters\n",
    "    nn = StochNet(\n",
    "        nb_past_timesteps=1,\n",
    "        nb_features=nb_features,\n",
    "        nb_randomized_params=len(params_to_randomize),\n",
    "        project_folder=project_folder,\n",
    "        timestep=timestep,\n",
    "        dataset_id=dataset_id,\n",
    "        model_id=model_id,\n",
    "        mode='inference'\n",
    "    )\n",
    "\n",
    "    n_settings = 500\n",
    "    traj_per_setting = 10\n",
    "    n_steps = 50\n",
    "\n",
    "    m = CRN_class(endtime, timestep)\n",
    "\n",
    "    initial_settings = m.get_initial_settings(n_settings)\n",
    "    randomized_params = m.get_randomized_parameters(params_to_randomize, n_settings)\n",
    "    settings = merge_species_and_param_settings(initial_settings, randomized_params)\n",
    "\n",
    "    # Get the current state to be fed into the network\n",
    "    setting_idx = np.random.randint(0, n_settings)\n",
    "    curr_state = settings[setting_idx:setting_idx+1, np.newaxis, :]\n",
    "    print(f\"Current state shape: {curr_state.shape}\")\n",
    "    print(f\"Current state: {curr_state}\")\n",
    "    \n",
    "    # Predict the next state\n",
    "    next_state_samples = nn.next_state(\n",
    "        curr_state_values=curr_state,\n",
    "        curr_state_rescaled=False,\n",
    "        scale_back_result=True,\n",
    "        round_result=False,\n",
    "        n_samples=10000,\n",
    "    )\n",
    "\n",
    "    random_sample_idx = np.random.randint(0, 10000)\n",
    "\n",
    "    print(f\"Shape: {next_state_samples.shape}\")\n",
    "    print(f\"Random sample index: {random_sample_idx}\")\n",
    "    print(f\"Random concentration prediction:\\n{next_state_samples[random_sample_idx, :, :, :]}\")\n",
    "    \n",
    "    # Visualise the distribution of concentration predictions for a single species across all samples\n",
    "    species = CRN_class.get_species_for_histogram()\n",
    "    for i, s in enumerate(species):\n",
    "        samples = np.squeeze(next_state_samples, -2)[..., i]\n",
    "        _ = plt.hist(samples, bins=50)\n",
    "    plt.legend(species)\n",
    "    plt.savefig(fr\"{name}\\{name}-concetrations.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Run and time a gillespy2 simulation\n",
    "    start = time()\n",
    "\n",
    "    gillespy_traces = generate_gillespy_traces(\n",
    "        settings=settings,\n",
    "        n_steps=n_steps,\n",
    "        timestep=timestep,\n",
    "        gillespy_model=m,\n",
    "        params_to_randomize=params_to_randomize,\n",
    "        traj_per_setting=traj_per_setting,\n",
    "    )\n",
    "\n",
    "    gillespy_time = time() - start\n",
    "\n",
    "    # Run and time a neural network simulation\n",
    "    start = time()\n",
    "\n",
    "    nn_traces = nn.generate_traces(\n",
    "        settings[:, np.newaxis, :],\n",
    "        n_steps=n_steps,\n",
    "        n_traces=traj_per_setting,\n",
    "        curr_state_rescaled=False,\n",
    "        scale_back_result=True,\n",
    "        round_result=True,\n",
    "        add_timestamps=True,\n",
    "    )\n",
    "\n",
    "    nn_time = time() - start\n",
    "    \n",
    "    print(f\"Gillespy2 shape and time: {gillespy_traces.shape, gillespy_time}\")\n",
    "    print(f\"StochNetV2 shape and time: {nn_traces.shape, nn_time}\")\n",
    "    \n",
    "    k = 1\n",
    "    n_traces = 1\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plot_random_traces(gillespy_traces[k][...,:nb_features+1], n_traces, linestyle='--', marker='')\n",
    "    plot_random_traces(nn_traces[k], n_traces, linestyle='-', marker='')\n",
    "    plt.savefig(fr\"{name}\\{name}-ssa-vs-nn-traces.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    new_traces = np.reshape(nn_traces, (traj_per_setting*n_settings, n_steps+1, n_species+1))\n",
    "    print(f\"new traces shape: {new_traces.shape}\")\n",
    "        \n",
    "    print(f\">>> FINISHED WITH {name}\")\n",
    "    print(f\"Generated traces shape: {new_traces.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "exercise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
